{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70d5fc9d-e1a7-4ec8-a541-154b7f3dd134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def demographic_parity_difference(predictions, sensitive_attribute):\n",
    "\n",
    "    # convert preds and sensitive attribute to arrays\n",
    "    predictions = np.array(predictions)\n",
    "    sensitive_attribute = np.array(sensitive_attribute)\n",
    "    \n",
    "    # get labels + sensitive attribute vals\n",
    "    unique_labels = np.unique(predictions)\n",
    "    unique_sensitive_attribute = np.unique(sensitive_attribute)\n",
    "\n",
    "    # print(unique_labels)\n",
    "    # print(sensitive_attribute)\n",
    "    \n",
    "    # store dpd\n",
    "    dp_diff = np.zeros(len(unique_labels))\n",
    "    privileged_gender = np.empty(len(unique_labels), dtype=object)\n",
    "\n",
    "    \n",
    "    # compute dpd for each label\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        # find indices where pred is equal to current label\n",
    "        label_indices = np.where(predictions == label)[0]\n",
    "        \n",
    "        # compute proportions of positive outcomes for each group\n",
    "        group_proportions = []\n",
    "        for a in unique_sensitive_attribute:\n",
    "            # find indices where sensitive attribute is equal to current value\n",
    "            group_indices = np.where(sensitive_attribute == a)[0]\n",
    "            \n",
    "            # compute proportion of positive outcomes for this group\n",
    "            positive_proportion = np.mean(predictions[group_indices] == label)\n",
    "            group_proportions.append(positive_proportion)\n",
    "        \n",
    "        # compute dpd for this label\n",
    "        dp_diff[i] = max(group_proportions) - min(group_proportions)\n",
    "\n",
    "        # ID privileged gender for this label\n",
    "        privileged_index = np.argmax(group_proportions)\n",
    "        privileged_gender[i] = unique_sensitive_attribute[privileged_index]\n",
    "    \n",
    "    return dp_diff, privileged_gender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d82d52fb-b725-4799-b4b1-b3a8d254a4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def equal_opportunity_difference(predictions, true_labels, sensitive_attribute):\n",
    "\n",
    "    # convert preds and sensitive attribute to arrays\n",
    "    predictions = np.array(predictions)\n",
    "    true_labels = np.array(true_labels)\n",
    "    sensitive_attribute = np.array(sensitive_attribute)\n",
    "    \n",
    "    # get labels + sensitive attribute vals\n",
    "    unique_labels = np.unique(predictions)\n",
    "    unique_sensitive_attribute = np.unique(sensitive_attribute)\n",
    "    \n",
    "    # store eod\n",
    "    eo_diff = np.zeros(len(unique_labels))\n",
    "    privileged_gender = np.empty(len(unique_labels), dtype=object)\n",
    "    \n",
    "    # compute eod for each label\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        # find indices where pred is equal to current label\n",
    "        label_indices = np.where(predictions == label)[0]\n",
    "        \n",
    "        # compute TPR for each group\n",
    "        tpr_group = []\n",
    "        for a in unique_sensitive_attribute:\n",
    "            # find indices where sensitive attribute is equal to current value\n",
    "            group_indices = np.where(sensitive_attribute == a)[0]\n",
    "            \n",
    "            # compute TPR for this group\n",
    "            true_positives = np.sum((predictions[group_indices] == label) & (true_labels[group_indices] == label))\n",
    "            actual_positives = np.sum(true_labels[group_indices] == label)\n",
    "            \n",
    "            if actual_positives > 0:\n",
    "                true_positive_rate = true_positives / actual_positives\n",
    "                tpr_group.append(true_positive_rate)\n",
    "            else:\n",
    "                tpr_group.append(0.0)  # division by zero case\n",
    "        \n",
    "        # compute eod for this label\n",
    "        eo_diff[i] = max(tpr_group) - min(tpr_group)\n",
    "        \n",
    "        # ID privileged gender for this label\n",
    "        privileged_index = np.argmax(tpr_group)\n",
    "        privileged_gender[i] = unique_sensitive_attribute[privileged_index]\n",
    "    \n",
    "    return eo_diff, privileged_gender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "498ec2a2-9e22-4484-9696-23a75d01acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def average_odds_difference(predictions, true_labels, sensitive_attribute):\n",
    "\n",
    "    # convert preds and sensitive attribute to arrays\n",
    "    predictions = np.array(predictions)\n",
    "    true_labels = np.array(true_labels)\n",
    "    sensitive_attribute = np.array(sensitive_attribute)\n",
    "    \n",
    "    # get labels + sensitive attribute vals\n",
    "    unique_labels = np.unique(predictions)\n",
    "    unique_sensitive_attribute = np.unique(sensitive_attribute)\n",
    "    \n",
    "    # store aod\n",
    "    aod_diff = np.zeros(len(unique_labels))\n",
    "    privileged_gender = np.empty(len(unique_labels), dtype=object)\n",
    "    \n",
    "    # compute aod for each label\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        # find indices where pred is equal to current label\n",
    "        label_indices = np.where(predictions == label)[0]\n",
    "        \n",
    "        # compute FPR and TPR for each group\n",
    "        fpr_group = []\n",
    "        tpr_group = []\n",
    "        for a in unique_sensitive_attribute:\n",
    "            # find indices where sensitive attribute is equal to current value\n",
    "            group_indices = np.where(sensitive_attribute == a)[0]\n",
    "            \n",
    "            # compute FPR for this group\n",
    "            false_positives = np.sum((predictions[group_indices] == label) & (true_labels[group_indices] != label))\n",
    "            actual_negatives = np.sum(true_labels[group_indices] != label)\n",
    "            if actual_negatives > 0:\n",
    "                false_positive_rate = false_positives / actual_negatives\n",
    "                fpr_group.append(false_positive_rate)\n",
    "            else:\n",
    "                fpr_group.append(0.0)  # division by zero case\n",
    "            \n",
    "            # compute TPR for this group\n",
    "            true_positives = np.sum((predictions[group_indices] == label) & (true_labels[group_indices] == label))\n",
    "            actual_positives = np.sum(true_labels[group_indices] == label)\n",
    "            if actual_positives > 0:\n",
    "                true_positive_rate = true_positives / actual_positives\n",
    "                tpr_group.append(true_positive_rate)\n",
    "            else:\n",
    "                tpr_group.append(0.0)  # division by zero case\n",
    "        \n",
    "        # compute aod for this label\n",
    "        aod_diff[i] = (max(fpr_group) - min(fpr_group) + max(tpr_group) - min(tpr_group)) / 2\n",
    "        \n",
    "        # ID privileged gender for this label\n",
    "        privileged_index = np.argmax(tpr_group)\n",
    "        privileged_gender[i] = unique_sensitive_attribute[privileged_index]\n",
    "    \n",
    "    return aod_diff, privileged_gender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "801975e2-2d22-4899-9014-2d7e2ac08e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def disparate_impact(predictions, sensitive_attribute):\n",
    "\n",
    "    # convert preds and sensitive attribute to arrays\n",
    "    predictions = np.array(predictions)\n",
    "    sensitive_attribute = np.array(sensitive_attribute)\n",
    "    \n",
    "    # get labels + sensitive attribute vals\n",
    "    unique_labels = np.unique(predictions)\n",
    "    unique_sensitive_attribute = np.unique(sensitive_attribute)\n",
    "    \n",
    "    # store di ratio\n",
    "    di_ratio = np.zeros(len(unique_labels))\n",
    "    privileged_gender = np.empty(len(unique_labels), dtype=object)\n",
    "    \n",
    "    # compute di ratio for each label\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        # find indices where pred is equal to current label\n",
    "        label_indices = np.where(predictions == label)[0]\n",
    "        \n",
    "        # compute proportions of positive outcomes for each group\n",
    "        favorable_proportions = []\n",
    "        for a in unique_sensitive_attribute:\n",
    "            # find indices where sensitive attribute is equal to current value\n",
    "            group_indices = np.where(sensitive_attribute == a)[0]\n",
    "            \n",
    "            # compute proportion of positive outcomes for this group\n",
    "            favorable_proportion = np.mean(predictions[group_indices] == label)\n",
    "            favorable_proportions.append(favorable_proportion)\n",
    "        \n",
    "        # compute DI ratio for this label\n",
    "        di_ratio[i] = favorable_proportions[1] / favorable_proportions[0] if favorable_proportions[0] != 0 else 0\n",
    "        \n",
    "        # ID privileged gender for this label\n",
    "        privileged_index = np.argmax(favorable_proportions)\n",
    "        privileged_gender[i] = unique_sensitive_attribute[privileged_index]\n",
    "    \n",
    "    return di_ratio, privileged_gender\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b349bbb3-5ac0-4baa-a3a3-cea4f4cf4130",
   "metadata": {},
   "source": [
    "Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "466888b0-066d-492f-bac1-4a20883410c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess biased dataset\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "data = pd.read_csv('biased.csv')\n",
    "\n",
    "# drop rows with missing values in text columns\n",
    "text_columns = ['Name', 'Education', 'Work Experience', 'Skills', 'Awards', 'Job']\n",
    "data = data.dropna(subset=text_columns)\n",
    "\n",
    "# extract text data and labels\n",
    "X_text = data['Name'] + ' ' + data['Education'] + ' ' + data['Work Experience'] + ' ' + data['Skills'] + ' ' + data['Awards']\n",
    "y = data['Fit']\n",
    "gender = data['Gender']\n",
    "\n",
    "# vectorize text data with TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_text_vec = vectorizer.fit_transform(X_text)\n",
    "\n",
    "# split dataset\n",
    "X_train, X_test, y_train, y_test, gender_train, gender_test = train_test_split(X_text_vec, y, gender, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363d73ee-7cd3-4058-8b19-d1725cb962e6",
   "metadata": {},
   "source": [
    "SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1177b06f-fa7d-4fa3-af3c-e2b7f93619e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic parity difference for each label: (array([0.02680653, 0.01759907, 0.10291375, 0.12703963, 0.18554779]), array(['male', 'male', 'female', 'female', 'male'], dtype=object))\n",
      "Equal opportunity difference for each label: (array([0.31818182, 0.26785714, 0.06868687, 0.58536585, 0.16304348]), array(['male', 'female', 'female', 'female', 'male'], dtype=object))\n",
      "Average odds difference for each label: (array([0.17000985, 0.13587245, 0.06207184, 0.30686642, 0.13155894]), array(['male', 'female', 'female', 'female', 'male'], dtype=object))\n",
      "Disparate impact for each label: (array([1.11057692, 1.12226721, 0.66866792, 0.47596154, 4.06153846]), array(['male', 'male', 'female', 'female', 'male'], dtype=object))\n",
      "\n",
      "Accuracy: 0.59\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     average       0.46      0.65      0.54        48\n",
      "         bad       0.78      0.70      0.74        44\n",
      "        good       0.71      0.62      0.66        78\n",
      "    very bad       0.51      0.57      0.54        42\n",
      "   very good       0.50      0.40      0.44        50\n",
      "\n",
      "    accuracy                           0.59       262\n",
      "   macro avg       0.59      0.59      0.58       262\n",
      "weighted avg       0.60      0.59      0.59       262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train SVM\n",
    "\n",
    "svm_classifier = SVC(kernel='linear', C=1.0, random_state=42) \n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "dp_diff = demographic_parity_difference(y_pred, gender_test)\n",
    "print(\"Demographic parity difference for each label:\", dp_diff)\n",
    "\n",
    "eo_diff = equal_opportunity_difference(y_pred, y_test, gender_test)\n",
    "print(\"Equal opportunity difference for each label:\", eo_diff)\n",
    "\n",
    "ao_diff = average_odds_difference(y_pred, y_test, gender_test)\n",
    "print(\"Average odds difference for each label:\", ao_diff)\n",
    "\n",
    "di_ratio = disparate_impact(y_pred, gender_test)\n",
    "print(\"Disparate impact for each label:\", di_ratio)\n",
    "\n",
    "# evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bcf6cb-a500-4f7d-9ab5-b8ecc4021ca3",
   "metadata": {},
   "source": [
    "Log Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "403d300a-c265-42d3-8146-e068e51913af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic parity difference for each label: (array([0.05757576, 0.01759907, 0.11818182, 0.12692308, 0.16993007]), array(['male', 'male', 'female', 'female', 'male'], dtype=object))\n",
      "Equal opportunity difference for each label: (array([0.3951049 , 0.26785714, 0.07676768, 0.58536585, 0.39130435]), array(['male', 'female', 'female', 'female', 'male'], dtype=object))\n",
      "Average odds difference for each label: (array([0.19938683, 0.13587245, 0.0776065 , 0.30524788, 0.23173848]), array(['male', 'female', 'female', 'female', 'male'], dtype=object))\n",
      "Disparate impact for each label: (array([1.2375    , 1.12226721, 0.62857143, 0.49230769, 4.73846154]), array(['male', 'male', 'female', 'female', 'male'], dtype=object))\n",
      "Accuracy: 0.56\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     average       0.41      0.60      0.49        48\n",
      "         bad       0.78      0.70      0.74        44\n",
      "        good       0.68      0.59      0.63        78\n",
      "    very bad       0.49      0.57      0.53        42\n",
      "   very good       0.53      0.36      0.43        50\n",
      "\n",
      "    accuracy                           0.56       262\n",
      "   macro avg       0.58      0.57      0.56       262\n",
      "weighted avg       0.59      0.56      0.57       262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train the Logistic Regression classifier\n",
    "\n",
    "logistic_classifier = LogisticRegression(max_iter=1000, random_state=42)  \n",
    "logistic_classifier.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "y_pred = logistic_classifier.predict(X_test)\n",
    "\n",
    "dp_diff = demographic_parity_difference(y_pred, gender_test)\n",
    "print(\"Demographic parity difference for each label:\", dp_diff)\n",
    "\n",
    "eo_diff = equal_opportunity_difference(y_pred, y_test, gender_test)\n",
    "print(\"Equal opportunity difference for each label:\", eo_diff)\n",
    "\n",
    "ao_diff = average_odds_difference(y_pred, y_test, gender_test)\n",
    "print(\"Average odds difference for each label:\", ao_diff)\n",
    "\n",
    "di_ratio = disparate_impact(y_pred, gender_test)\n",
    "print(\"Disparate impact for each label:\", di_ratio)\n",
    "\n",
    "\n",
    "# evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab8229c-3057-4547-abaa-ded376765629",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cbcdf10c-53ff-48d8-8ca0-687ae8d10028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic parity difference for each label: (array([0.01060606, 0.0488345 , 0.09452214, 0.08146853, 0.11655012]), array(['male', 'male', 'female', 'female', 'male'], dtype=object))\n",
      "Equal opportunity difference for each label: (array([0.26223776, 0.08035714, 0.01010101, 0.53658537, 0.04347826]), array(['male', 'female', 'female', 'female', 'female'], dtype=object))\n",
      "Average odds difference for each label: (array([0.1409896 , 0.05015153, 0.01950719, 0.30283566, 0.02917961]), array(['male', 'female', 'female', 'female', 'female'], dtype=object))\n",
      "Disparate impact for each label: (array([1.056     , 1.28026756, 0.73453355, 0.6017094 , 2.53846154]), array(['male', 'male', 'female', 'female', 'male'], dtype=object))\n",
      "Accuracy: 0.73\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     average       0.76      0.81      0.79        48\n",
      "         bad       0.75      0.89      0.81        44\n",
      "        good       0.85      0.88      0.87        78\n",
      "    very bad       0.51      0.52      0.52        42\n",
      "   very good       0.66      0.46      0.54        50\n",
      "\n",
      "    accuracy                           0.73       262\n",
      "   macro avg       0.71      0.71      0.71       262\n",
      "weighted avg       0.73      0.73      0.73       262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train Random Forest\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)  \n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "dp_diff = demographic_parity_difference(y_pred, gender_test)\n",
    "print(\"Demographic parity difference for each label:\", dp_diff)\n",
    "\n",
    "eo_diff = equal_opportunity_difference(y_pred, y_test, gender_test)\n",
    "print(\"Equal opportunity difference for each label:\", eo_diff)\n",
    "\n",
    "ao_diff = average_odds_difference(y_pred, y_test, gender_test)\n",
    "print(\"Average odds difference for each label:\", ao_diff)\n",
    "\n",
    "di_ratio = disparate_impact(y_pred, gender_test)\n",
    "print(\"Disparate impact for each label:\", di_ratio)\n",
    "\n",
    "# evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3b33dd-aceb-40e4-a6d8-f2301499a7f3",
   "metadata": {},
   "source": [
    "Grad Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a787ca37-c972-4d1c-b12a-a73d44c6cb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic parity difference for each label: (array([0.05652681, 0.03310023, 0.10291375, 0.09615385, 0.10944056]), array(['male', 'male', 'female', 'female', 'male'], dtype=object))\n",
      "Equal opportunity difference for each label: (array([0.13286713, 0.15178571, 0.05050505, 0.58536585, 0.04347826]), array(['male', 'female', 'female', 'female', 'male'], dtype=object))\n",
      "Average odds difference for each label: (array([0.09849157, 0.08764066, 0.04427137, 0.32075176, 0.04871086]), array(['male', 'female', 'female', 'female', 'male'], dtype=object))\n",
      "Disparate impact for each label: (array([1.32441472, 1.21846154, 0.66866792, 0.61538462, 1.96307692]), array(['male', 'male', 'female', 'female', 'male'], dtype=object))\n",
      "Accuracy: 0.71\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     average       0.72      0.79      0.75        48\n",
      "         bad       0.84      0.84      0.84        44\n",
      "        good       0.87      0.76      0.81        78\n",
      "    very bad       0.45      0.57      0.51        42\n",
      "   very good       0.61      0.54      0.57        50\n",
      "\n",
      "    accuracy                           0.71       262\n",
      "   macro avg       0.70      0.70      0.70       262\n",
      "weighted avg       0.72      0.71      0.71       262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train Gradient Boosting classifier\n",
    "\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "y_pred = gb_classifier.predict(X_test)\n",
    "\n",
    "dp_diff = demographic_parity_difference(y_pred, gender_test)\n",
    "print(\"Demographic parity difference for each label:\", dp_diff)\n",
    "\n",
    "eo_diff = equal_opportunity_difference(y_pred, y_test, gender_test)\n",
    "print(\"Equal opportunity difference for each label:\", eo_diff)\n",
    "\n",
    "ao_diff = average_odds_difference(y_pred, y_test, gender_test)\n",
    "print(\"Average odds difference for each label:\", ao_diff)\n",
    "\n",
    "di_ratio = disparate_impact(y_pred, gender_test)\n",
    "print(\"Disparate impact for each label:\", di_ratio)\n",
    "\n",
    "# evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21201d3c-38f0-4126-9a01-4e344021c033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
