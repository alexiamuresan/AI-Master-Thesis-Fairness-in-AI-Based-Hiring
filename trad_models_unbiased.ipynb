{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70d5fc9d-e1a7-4ec8-a541-154b7f3dd134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def demographic_parity_difference(predictions, sensitive_attribute):\n",
    "\n",
    "    # convert preds and sensitive attribute to arrays\n",
    "    predictions = np.array(predictions)\n",
    "    sensitive_attribute = np.array(sensitive_attribute)\n",
    "    \n",
    "    # get labels + sensitive attribute vals\n",
    "    unique_labels = np.unique(predictions)\n",
    "    unique_sensitive_attribute = np.unique(sensitive_attribute)\n",
    "\n",
    "    # print(unique_labels)\n",
    "    # print(sensitive_attribute)\n",
    "    \n",
    "    # store dpd\n",
    "    dp_diff = np.zeros(len(unique_labels))\n",
    "    privileged_gender = np.empty(len(unique_labels), dtype=object)\n",
    "\n",
    "    \n",
    "    # compute dpd for each label\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        # find indices where pred is equal to current label\n",
    "        label_indices = np.where(predictions == label)[0]\n",
    "        \n",
    "        # compute proportions of positive outcomes for each group\n",
    "        group_proportions = []\n",
    "        for a in unique_sensitive_attribute:\n",
    "            # find indices where sensitive attribute is equal to current value\n",
    "            group_indices = np.where(sensitive_attribute == a)[0]\n",
    "            \n",
    "            # compute proportion of positive outcomes for this group\n",
    "            positive_proportion = np.mean(predictions[group_indices] == label)\n",
    "            group_proportions.append(positive_proportion)\n",
    "        \n",
    "        # compute dpd for this label\n",
    "        dp_diff[i] = max(group_proportions) - min(group_proportions)\n",
    "\n",
    "        # ID privileged gender for this label\n",
    "        privileged_index = np.argmax(group_proportions)\n",
    "        privileged_gender[i] = unique_sensitive_attribute[privileged_index]\n",
    "    \n",
    "    return dp_diff, privileged_gender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21ab4872-b2a9-48b2-9597-57e4b41ad419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def equal_opportunity_difference(predictions, true_labels, sensitive_attribute):\n",
    "\n",
    "    # convert preds and sensitive attribute to arrays\n",
    "    predictions = np.array(predictions)\n",
    "    true_labels = np.array(true_labels)\n",
    "    sensitive_attribute = np.array(sensitive_attribute)\n",
    "    \n",
    "    # get labels + sensitive attribute vals\n",
    "    unique_labels = np.unique(predictions)\n",
    "    unique_sensitive_attribute = np.unique(sensitive_attribute)\n",
    "    \n",
    "    # store eod\n",
    "    eo_diff = np.zeros(len(unique_labels))\n",
    "    privileged_gender = np.empty(len(unique_labels), dtype=object)\n",
    "    \n",
    "    # compute eod for each label\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        # find indices where pred is equal to current label\n",
    "        label_indices = np.where(predictions == label)[0]\n",
    "        \n",
    "        # compute TPR for each group\n",
    "        tpr_group = []\n",
    "        for a in unique_sensitive_attribute:\n",
    "            # find indices where sensitive attribute is equal to current value\n",
    "            group_indices = np.where(sensitive_attribute == a)[0]\n",
    "            \n",
    "            # compute TPR for this group\n",
    "            true_positives = np.sum((predictions[group_indices] == label) & (true_labels[group_indices] == label))\n",
    "            actual_positives = np.sum(true_labels[group_indices] == label)\n",
    "            \n",
    "            if actual_positives > 0:\n",
    "                true_positive_rate = true_positives / actual_positives\n",
    "                tpr_group.append(true_positive_rate)\n",
    "            else:\n",
    "                tpr_group.append(0.0)  # division by zero case\n",
    "        \n",
    "        # compute eod for this label\n",
    "        eo_diff[i] = max(tpr_group) - min(tpr_group)\n",
    "        \n",
    "        # ID privileged gender for this label\n",
    "        privileged_index = np.argmax(tpr_group)\n",
    "        privileged_gender[i] = unique_sensitive_attribute[privileged_index]\n",
    "    \n",
    "    return eo_diff, privileged_gender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9130df40-3425-4005-896a-b5b18dc8a46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def average_odds_difference(predictions, true_labels, sensitive_attribute):\n",
    "\n",
    "    # convert preds and sensitive attribute to arrays\n",
    "    predictions = np.array(predictions)\n",
    "    true_labels = np.array(true_labels)\n",
    "    sensitive_attribute = np.array(sensitive_attribute)\n",
    "    \n",
    "    # get labels + sensitive attribute vals\n",
    "    unique_labels = np.unique(predictions)\n",
    "    unique_sensitive_attribute = np.unique(sensitive_attribute)\n",
    "    \n",
    "    # store aod\n",
    "    aod_diff = np.zeros(len(unique_labels))\n",
    "    privileged_gender = np.empty(len(unique_labels), dtype=object)\n",
    "    \n",
    "    # compute aod for each label\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        # find indices where pred is equal to current label\n",
    "        label_indices = np.where(predictions == label)[0]\n",
    "        \n",
    "        # compute FPR and TPR for each group\n",
    "        fpr_group = []\n",
    "        tpr_group = []\n",
    "        for a in unique_sensitive_attribute:\n",
    "            # find indices where sensitive attribute is equal to current value\n",
    "            group_indices = np.where(sensitive_attribute == a)[0]\n",
    "            \n",
    "            # compute FPR for this group\n",
    "            false_positives = np.sum((predictions[group_indices] == label) & (true_labels[group_indices] != label))\n",
    "            actual_negatives = np.sum(true_labels[group_indices] != label)\n",
    "            if actual_negatives > 0:\n",
    "                false_positive_rate = false_positives / actual_negatives\n",
    "                fpr_group.append(false_positive_rate)\n",
    "            else:\n",
    "                fpr_group.append(0.0)  # division by zero case\n",
    "            \n",
    "            # compute TPR for this group\n",
    "            true_positives = np.sum((predictions[group_indices] == label) & (true_labels[group_indices] == label))\n",
    "            actual_positives = np.sum(true_labels[group_indices] == label)\n",
    "            if actual_positives > 0:\n",
    "                true_positive_rate = true_positives / actual_positives\n",
    "                tpr_group.append(true_positive_rate)\n",
    "            else:\n",
    "                tpr_group.append(0.0)  # division by zero case\n",
    "        \n",
    "        # compute aod for this label\n",
    "        aod_diff[i] = (max(fpr_group) - min(fpr_group) + max(tpr_group) - min(tpr_group)) / 2\n",
    "        \n",
    "        # ID privileged gender for this label\n",
    "        privileged_index = np.argmax(tpr_group)\n",
    "        privileged_gender[i] = unique_sensitive_attribute[privileged_index]\n",
    "    \n",
    "    return aod_diff, privileged_gender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef43395a-f937-4122-bdb2-86db4d8ba6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def disparate_impact(predictions, sensitive_attribute):\n",
    "\n",
    "    # convert preds and sensitive attribute to arrays\n",
    "    predictions = np.array(predictions)\n",
    "    sensitive_attribute = np.array(sensitive_attribute)\n",
    "    \n",
    "    # get labels + sensitive attribute vals\n",
    "    unique_labels = np.unique(predictions)\n",
    "    unique_sensitive_attribute = np.unique(sensitive_attribute)\n",
    "    \n",
    "    # store di ratio\n",
    "    di_ratio = np.zeros(len(unique_labels))\n",
    "    privileged_gender = np.empty(len(unique_labels), dtype=object)\n",
    "    \n",
    "    # compute di ratio for each label\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        # find indices where pred is equal to current label\n",
    "        label_indices = np.where(predictions == label)[0]\n",
    "        \n",
    "        # compute proportions of positive outcomes for each group\n",
    "        favorable_proportions = []\n",
    "        for a in unique_sensitive_attribute:\n",
    "            # find indices where sensitive attribute is equal to current value\n",
    "            group_indices = np.where(sensitive_attribute == a)[0]\n",
    "            \n",
    "            # compute proportion of positive outcomes for this group\n",
    "            favorable_proportion = np.mean(predictions[group_indices] == label)\n",
    "            favorable_proportions.append(favorable_proportion)\n",
    "        \n",
    "        # compute DI ratio for this label\n",
    "        di_ratio[i] = favorable_proportions[1] / favorable_proportions[0] if favorable_proportions[0] != 0 else 0\n",
    "        \n",
    "        # ID privileged gender for this label\n",
    "        privileged_index = np.argmax(favorable_proportions)\n",
    "        privileged_gender[i] = unique_sensitive_attribute[privileged_index]\n",
    "    \n",
    "    return di_ratio, privileged_gender\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b349bbb3-5ac0-4baa-a3a3-cea4f4cf4130",
   "metadata": {},
   "source": [
    "Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "466888b0-066d-492f-bac1-4a20883410c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "data = pd.read_csv('thesis_dataset.csv')\n",
    "\n",
    "# drop rows with missing values in text columns\n",
    "text_columns = ['Name', 'Education', 'Work Experience', 'Skills', 'Awards']\n",
    "data = data.dropna(subset=text_columns)\n",
    "\n",
    "# extract text data and labels\n",
    "X_text = data['Name'] + ' ' + data['Education'] + ' ' + data['Work Experience'] + ' ' + data['Skills'] + ' ' + data['Awards']\n",
    "y = data['Fit']\n",
    "gender = data['Gender']\n",
    "\n",
    "# vectorize the text data with TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_text_vec = vectorizer.fit_transform(X_text)\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test, gender_train, gender_test = train_test_split(X_text_vec, y, gender, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363d73ee-7cd3-4058-8b19-d1725cb962e6",
   "metadata": {},
   "source": [
    "SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1177b06f-fa7d-4fa3-af3c-e2b7f93619e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic parity difference for each label: (array([0.06445221, 0.03275058, 0.12587413, 0.03286713, 0.0041958 ]), array(['male', 'male', 'female', 'male', 'female'], dtype=object))\n",
      "Equal opportunity difference for each label: (array([0.27972028, 0.13392857, 0.12121212, 0.08235294, 0.07333333]), array(['male', 'female', 'female', 'male', 'female'], dtype=object))\n",
      "Average odds difference for each label: (array([0.15741779, 0.07440174, 0.0934891 , 0.06322135, 0.04872274]), array(['male', 'female', 'female', 'male', 'female'], dtype=object))\n",
      "Disparate impact for each label: (array([1.34030769, 1.25429864, 0.6043956 , 1.24102564, 0.98153846]), array(['male', 'male', 'female', 'male', 'female'], dtype=object))\n",
      "\n",
      "Accuracy: 0.71\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     average       0.55      0.67      0.60        48\n",
      "         bad       0.84      0.73      0.78        44\n",
      "        good       0.72      0.62      0.66        78\n",
      "    very bad       0.78      0.84      0.81        37\n",
      "   very good       0.75      0.80      0.77        55\n",
      "\n",
      "    accuracy                           0.71       262\n",
      "   macro avg       0.73      0.73      0.72       262\n",
      "weighted avg       0.72      0.71      0.71       262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train SVM\n",
    "\n",
    "svm_classifier = SVC(kernel='linear', C=1.0, random_state=42) \n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "dp_diff = demographic_parity_difference(y_pred, gender_test)\n",
    "print(\"Demographic parity difference for each label:\", dp_diff)\n",
    "\n",
    "eo_diff = equal_opportunity_difference(y_pred, y_test, gender_test)\n",
    "print(\"Equal opportunity difference for each label:\", eo_diff)\n",
    "\n",
    "ao_diff = average_odds_difference(y_pred, y_test, gender_test)\n",
    "print(\"Average odds difference for each label:\", ao_diff)\n",
    "\n",
    "di_ratio = disparate_impact(y_pred, gender_test)\n",
    "print(\"Disparate impact for each label:\", di_ratio)\n",
    "\n",
    "# evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bcf6cb-a500-4f7d-9ab5-b8ecc4021ca3",
   "metadata": {},
   "source": [
    "Log Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "403d300a-c265-42d3-8146-e068e51913af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['average' 'bad' 'good' 'very bad' 'very good']\n",
      "Demographic parity difference for each label: (array([0.09452214, 0.04801865, 0.11095571, 0.002331  , 0.03391608]), array(['male', 'male', 'female', 'male', 'female'], dtype=object))\n",
      "Equal opportunity difference for each label: (array([0.34265734, 0.10714286, 0.08484848, 0.03529412, 0.08666667]), array(['male', 'female', 'female', 'female', 'female'], dtype=object))\n",
      "Average odds difference for each label: (array([0.19832029, 0.05593789, 0.0747148 , 0.03076337, 0.07309969]), array(['male', 'female', 'female', 'female', 'female'], dtype=object))\n",
      "Disparate impact for each label: (array([1.65668016, 1.39615385, 0.6145749 , 1.01538462, 0.8852071 ]), array(['male', 'male', 'female', 'male', 'female'], dtype=object))\n",
      "Accuracy: 0.68\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     average       0.52      0.54      0.53        48\n",
      "         bad       0.79      0.68      0.73        44\n",
      "        good       0.72      0.56      0.63        78\n",
      "    very bad       0.72      0.78      0.75        37\n",
      "   very good       0.66      0.87      0.75        55\n",
      "\n",
      "    accuracy                           0.68       262\n",
      "   macro avg       0.68      0.69      0.68       262\n",
      "weighted avg       0.68      0.68      0.67       262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train the Logistic Regression classifier\n",
    "\n",
    "logistic_classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logistic_classifier.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "y_pred = logistic_classifier.predict(X_test)\n",
    "\n",
    "dp_diff = demographic_parity_difference(y_pred, gender_test)\n",
    "print(\"Demographic parity difference for each label:\", dp_diff)\n",
    "\n",
    "eo_diff = equal_opportunity_difference(y_pred, y_test, gender_test)\n",
    "print(\"Equal opportunity difference for each label:\", eo_diff)\n",
    "\n",
    "ao_diff = average_odds_difference(y_pred, y_test, gender_test)\n",
    "print(\"Average odds difference for each label:\", ao_diff)\n",
    "\n",
    "di_ratio = disparate_impact(y_pred, gender_test)\n",
    "print(\"Disparate impact for each label:\", di_ratio)\n",
    "\n",
    "\n",
    "# evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab8229c-3057-4547-abaa-ded376765629",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cbcdf10c-53ff-48d8-8ca0-687ae8d10028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['average' 'bad' 'good' 'very bad' 'very good']\n",
      "Demographic parity difference for each label: (array([0.01759907, 0.01759907, 0.07983683, 0.04067599, 0.0039627 ]), array(['male', 'male', 'female', 'male', 'male'], dtype=object))\n",
      "Equal opportunity difference for each label: (array([0.20979021, 0.19642857, 0.07070707, 0.09117647, 0.00666667]), array(['male', 'female', 'male', 'male', 'male'], dtype=object))\n",
      "Average odds difference for each label: (array([0.10506981, 0.10996209, 0.05952702, 0.07205789, 0.02506231]), array(['male', 'female', 'male', 'male', 'male'], dtype=object))\n",
      "Disparate impact for each label: (array([1.12226721, 1.12226721, 0.74296435, 1.28259109, 1.01538462]), array(['male', 'male', 'female', 'male', 'male'], dtype=object))\n",
      "Accuracy: 0.83\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     average       0.90      0.75      0.82        48\n",
      "         bad       0.82      0.75      0.79        44\n",
      "        good       0.89      0.81      0.85        78\n",
      "    very bad       0.77      0.89      0.82        37\n",
      "   very good       0.78      0.96      0.86        55\n",
      "\n",
      "    accuracy                           0.83       262\n",
      "   macro avg       0.83      0.83      0.83       262\n",
      "weighted avg       0.84      0.83      0.83       262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train Random Forest\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "dp_diff = demographic_parity_difference(y_pred, gender_test)\n",
    "print(\"Demographic parity difference for each label:\", dp_diff)\n",
    "\n",
    "eo_diff = equal_opportunity_difference(y_pred, y_test, gender_test)\n",
    "print(\"Equal opportunity difference for each label:\", eo_diff)\n",
    "\n",
    "ao_diff = average_odds_difference(y_pred, y_test, gender_test)\n",
    "print(\"Average odds difference for each label:\", ao_diff)\n",
    "\n",
    "di_ratio = disparate_impact(y_pred, gender_test)\n",
    "print(\"Disparate impact for each label:\", di_ratio)\n",
    "\n",
    "# evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3b33dd-aceb-40e4-a6d8-f2301499a7f3",
   "metadata": {},
   "source": [
    "Grad Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a787ca37-c972-4d1c-b12a-a73d44c6cb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['average' 'bad' 'good' 'very bad' 'very good']\n",
      "Demographic parity difference for each label: (array([0.07960373, 0.05582751, 0.0955711 , 0.02062937, 0.01923077]), array(['male', 'male', 'female', 'female', 'female'], dtype=object))\n",
      "Equal opportunity difference for each label: (array([0.22377622, 0.08928571, 0.02828283, 0.06764706, 0.05333333]), array(['male', 'female', 'female', 'female', 'male'], dtype=object))\n",
      "Average odds difference for each label: (array([0.14857574, 0.04776997, 0.03197528, 0.03816929, 0.06806854]), array(['male', 'female', 'female', 'female', 'male'], dtype=object))\n",
      "Disparate impact for each label: (array([1.45685619, 1.43348416, 0.66801619, 0.87032967, 0.92307692]), array(['male', 'male', 'female', 'female', 'female'], dtype=object))\n",
      "Accuracy: 0.83\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     average       0.71      0.83      0.77        48\n",
      "         bad       0.88      0.82      0.85        44\n",
      "        good       0.92      0.74      0.82        78\n",
      "    very bad       0.87      0.92      0.89        37\n",
      "   very good       0.79      0.91      0.85        55\n",
      "\n",
      "    accuracy                           0.83       262\n",
      "   macro avg       0.84      0.84      0.84       262\n",
      "weighted avg       0.84      0.83      0.83       262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train Gradient Boosting classifier\n",
    "\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, random_state=42)  \n",
    "gb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "y_pred = gb_classifier.predict(X_test)\n",
    "\n",
    "dp_diff = demographic_parity_difference(y_pred, gender_test)\n",
    "print(\"Demographic parity difference for each label:\", dp_diff)\n",
    "\n",
    "eo_diff = equal_opportunity_difference(y_pred, y_test, gender_test)\n",
    "print(\"Equal opportunity difference for each label:\", eo_diff)\n",
    "\n",
    "ao_diff = average_odds_difference(y_pred, y_test, gender_test)\n",
    "print(\"Average odds difference for each label:\", ao_diff)\n",
    "\n",
    "di_ratio = disparate_impact(y_pred, gender_test)\n",
    "print(\"Disparate impact for each label:\", di_ratio)\n",
    "\n",
    "# evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21201d3c-38f0-4126-9a01-4e344021c033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
